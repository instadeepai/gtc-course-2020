{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: PPO\n",
    "\n",
    "* Let's repeat the run of the previous notebook, but with a smaller network and more time to run.\n",
    "* We are going to compare PPO to another algorithm.\n",
    "* Decide the size of network you want to train and run the cells below.\n",
    "* We will then move on to the next notebook while this model trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.env_utils import env_from_env_config\n",
    "from environments.observations import TreeObsForRailEnv\n",
    "from environments.preprocessor import TreeObsPreprocessor\n",
    "from models.dense_model import DenseModel\n",
    "from ray import tune\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to see many more iterations of training for PPO\n",
    "u =  # TODO: set this to something very small. Suggested: in the order of 10 units per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dense model with \"u\" hidden_layer units per layer\n",
    "model_params = {\n",
    "    \"embedding\": {\"hidden_layers\": [u, u], \"activation_fn\": \"relu\"},\n",
    "    \"actor\": {\"hidden_layers\": [u], \"activation_fn\": \"relu\"},\n",
    "    \"critic\": {\"hidden_layers\": [u], \"activation_fn\": \"relu\"},\n",
    "}\n",
    "custom_model = \"dense_model\"\n",
    "\n",
    "# Set up the environment\n",
    "env_config = {\n",
    "    \"obs_config\": {\"max_depth\": 2},\n",
    "    \"rail_generator\": \"complex_rail_generator\",\n",
    "    \"rail_config\": {\"nr_start_goal\": 12, \"nr_extra\": 0, \"min_dist\": 8, \"seed\": 10},\n",
    "    \"width\": 8,\n",
    "    \"height\": 8,\n",
    "    \"number_of_agents\": 5,\n",
    "    \"schedule_generator\": \"complex_schedule_generator\",\n",
    "    \"schedule_config\": {},\n",
    "    \"frozen\": False,\n",
    "    \"remove_agents_at_target\": True,\n",
    "    \"wait_for_all_done\": False\n",
    "}\n",
    "env = env_from_env_config(env_config)\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "# Define 1 policy per agent\n",
    "num_policies = env_config[\"number_of_agents\"]\n",
    "policies = {f\"policy_{i}\": (None, observation_space, action_space, {})\n",
    "            for i in range(num_policies)}\n",
    "\n",
    "# Register custom setup with RLlib\n",
    "register_env(\"train_env\", env_from_env_config)\n",
    "ModelCatalog.register_custom_model(custom_model, DenseModel)\n",
    "ModelCatalog.register_custom_preprocessor(\"tree_obs_preprocessor\", TreeObsPreprocessor)\n",
    "\n",
    "# Full experiment config\n",
    "config = {\n",
    "    # Run parameters\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_workers\": 7,\n",
    "    \"num_gpus\": 0,  # TODO: change this to 4\n",
    "    \n",
    "    # Environment parameters\n",
    "    \"env\": \"train_env\",\n",
    "    \"env_config\": env_config,\n",
    "    \"log_level\": \"ERROR\",\n",
    "    \n",
    "    # Training parameters\n",
    "    \"horizon\": 60,\n",
    "    \"num_sgd_iter\": 15,\n",
    "    \"lr\": 1e-4,\n",
    "    \n",
    "    # Policy parameters\n",
    "    \"vf_loss_coeff\": 1e-6,    \n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"policy_0\",\n",
    "    },\n",
    "    \n",
    "    # Model parameters\n",
    "    \"model\": {\n",
    "        \"custom_preprocessor\": \"tree_obs_preprocessor\",\n",
    "        \"custom_model\": custom_model,\n",
    "        \"custom_options\": {\n",
    "            \"tree_depth\": env_config[\"obs_config\"][\"max_depth\"],\n",
    "            \"observation_radius\": 0,\n",
    "            **model_params,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "n_GPUS = config[\"num_gpus\"]\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    name=f\"PPO_multi_agent-MODEL={custom_model}_{u}-GPUS={n_GPUS}\",\n",
    "    stop={\"training_iteration\": 80},\n",
    "    config=config,\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True,\n",
    "    loggers=tune.logger.DEFAULT_LOGGERS,\n",
    "    ray_auto_init=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fl_test] *",
   "language": "python",
   "name": "conda-env-fl_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
