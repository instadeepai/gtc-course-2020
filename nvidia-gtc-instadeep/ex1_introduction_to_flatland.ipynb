{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5H5GxUed8Di"
   },
   "source": [
    "# Introduction to Flatland Rail for MultiAgent Reinforcement Learning  \n",
    "\n",
    "In this notebook, we will learn how to:  \n",
    "   * Create a Flatland railway environment.  \n",
    "   * Build a random agent to take actions in the environment.\n",
    "   * Visualize a rollout of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flatland.envs.observations import GlobalObsForRailEnv\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from environments.custom_rail_generator import simple_rail_generator\n",
    "from environments.custom_schedule_generator import sparse_schedule_generator\n",
    "from environments.observations import TreeObsForRailEnv\n",
    "from environments.visualization_utils import animate_env, get_patch, render_env\n",
    "\n",
    "# Start virtual display before importing RenderTool\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "    \n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "First, we will set up the environment and discuss the observations available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Let's see how the environment looks with 3 trains\n",
    "n_trains = 3\n",
    "\n",
    "# 2. Set all trains to have the same constant speed\n",
    "speed_ration_map = {1.: 1.}\n",
    "\n",
    "# 3. Rail generator creates railway track in environment\n",
    "rail_generator = simple_rail_generator(n_trains=n_trains, seed=42)\n",
    "\n",
    "# 4. Schedule generator assigns starting positions and targets to trains\n",
    "schedule_generator = sparse_schedule_generator(speed_ration_map)\n",
    "\n",
    "# 5. Build the observation vectors for agents in the RailEnv environment - more on this later\n",
    "obs_builder_object = TreeObsForRailEnv(max_depth=2, predictor=None)\n",
    "\n",
    "env = RailEnv(\n",
    "            width=20,\n",
    "            height=8,\n",
    "            number_of_agents=n_trains,\n",
    "            rail_generator=rail_generator,\n",
    "            schedule_generator=schedule_generator,\n",
    "            obs_builder_object=obs_builder_object,\n",
    "            remove_agents_at_target=True,  # Removes agents at the end of their journey to make space for others\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXWGG9S1uS7y"
   },
   "outputs": [],
   "source": [
    "# Instantiate Renderer\n",
    "env_renderer = RenderTool(env, gl=\"PILSVG\",\n",
    "                          agent_render_variant=AgentRenderVariant.AGENT_SHOWS_OPTIONS_AND_BOX,\n",
    "                          show_debug=False,\n",
    "                          screen_height=726,\n",
    "                          screen_width=1240)\n",
    "env.reset()\n",
    "env_renderer.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how one snapshot of the environment looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_env(env_renderer, show_observations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations in Flatland form a tree structure.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.cloud.google.com/gtc-2020/images/flatland_obs.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Image from Flatland-rl-docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "Each node is filled with information gathered along the path to the node:\n",
    "\n",
    "1: If own target lies on the explored branch the current distance from the agent in number of cells is stored\n",
    "\n",
    "2: If another agent’s target is detected, the distance in number of cells from the current agent position is stored\n",
    "\n",
    "3: If another agent is detected, the distance in number of cells from the current agent position is stored\n",
    "\n",
    "4: Possible conflict detected (using a predictor)\n",
    "\n",
    "5: Distance to an unusable switch (for this agent), if detected. An unusable switch is a switch where the agent does not have any choice of path, but other agents coming from different directions might\n",
    "\n",
    "6: Distance (in number of cells) to the next node (e.g. switch or target or dead-end)\n",
    "\n",
    "7: Minimum remaining travel distance from this node to the agent’s target given the direction of the agent if this path is chosen\n",
    "\n",
    "8: Number of agents present in the same direction found on path to node\n",
    "\n",
    "9: Number of agents in the opposite direction on path to node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to visualize the observations available to each train in our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_env(env_renderer, show_observations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in real life, each train is not able to see the entire rail network, or fleet of other trains.  \n",
    "Here, each train can see a prescribed tree depth of observations ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n",
    "\n",
    "  * We have introduced the observation space.\n",
    "  * Now we will look at the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of actions available: \", env.action_space[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are up to 5 actions available at each step.  \n",
    "These are:  \n",
    "\n",
    "    0 Do Nothing: \n",
    "        If the train is already moving, it continous moving.  \n",
    "        If it is already stopped, it remains stopped.  \n",
    "        \n",
    "    1 Deviate Left: \n",
    "        If the train is at a switch with a transition to its left, the train will chose the left path.  \n",
    "        Otherwise this action has no effect.  \n",
    "        If the train is stopped, this action will start train movement again if allowed by the transitions.  \n",
    "        \n",
    "    2 Go Forward:\n",
    "        This action will start the train when stopped.  \n",
    "        This will move the agent forward and chose the go straight direction at switches.\n",
    "        \n",
    "    3 Deviate Right: \n",
    "        Same as deviate left but for right turns.\n",
    "        \n",
    "    4 Stop: \n",
    "        Causes the train to stop moving.\n",
    "        \n",
    "Run the cell below to see how the actions available lead to a decision tree when considering the next state into which the train can transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.cloud.google.com/gtc-2020/images/flatland_tree.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Image from Flatland-rl-docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards\n",
    "\n",
    "The following reward function is used in Flatland to give feedback to our agents:\n",
    "\n",
    "   * 'step_penalty' = -1: for every time-step taken in the environment, regardless of the action taken by the agent. Intuitively, this encourages the agent to finish as quickly as possible by taking the shortest path to its target.\n",
    "   * 'global_reward' = +1: every time an agent reaches its target destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rest of this notebook, we are going to create a simple agent that chooses a random action for each of the trains in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DAQxT1tuogt"
   },
   "outputs": [],
   "source": [
    "def random_agent(n_trains, n_actions):\n",
    "    \"\"\"Generates actions from a random policy.\"\"\"\n",
    "    action_dict = {}\n",
    "    for idx in range(n_trains):\n",
    "        action_dict[idx] = np.random.randint(0, n_actions)\n",
    "    return action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DAQxT1tuogt"
   },
   "outputs": [],
   "source": [
    "## Visualize random rollout steps in environment\n",
    "env.reset()\n",
    "env_renderer.reset()\n",
    "\n",
    "frames = []\n",
    "n_actions = env.action_space[0]\n",
    "for step in range(10):\n",
    "    # 1. Sample actions from agent\n",
    "    action_dict = random_agent(n_trains, n_actions)\n",
    "    \n",
    "    # 2. Each train takes a step in the environment at the same time\n",
    "    obs, rewards, done, info = env.step(action_dict)\n",
    "    \n",
    "    # 3. Render results. Change show_observations=True to see observations during the rollout\n",
    "    env_renderer.render_env(show=False, frames=False, show_observations=False)\n",
    "    frames.append(env_renderer.gl.get_image())\n",
    "    \n",
    "animate_env(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up Next: TensorFlow Models...\n",
    "\n",
    "In this section, we built a Flatland rail environment, investigated the observation and action spaces, and ran a random agent.\n",
    "\n",
    "In the next section, we will discuss how to use TensorFlow to build a neural network as a function approximator for mapping states to actions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "env_viz.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:fl_test] *",
   "language": "python",
   "name": "conda-env-fl_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
