{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollouts\n",
    "\n",
    "This notebook shows how to visualise a trained agent solving a new, unseen environment:\n",
    "- Load agent checkpoint\n",
    "- Load environment\n",
    "- Agent plays a game\n",
    "- Visualise step-by-step actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.visualization_utils import animate_env\n",
    "from utils.rollout import run_rollout\n",
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First see how PPO does.  \n",
    "You can use the path to your own trained model in the `ray_results` folder, or you can see the rollouts from a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO checkpoint: update this to be the path to your trained PPO model.\n",
    "# E.g. DIR = \"~/ray_results/.../\"\n",
    "#      CHECKPOINT = DIR + \"checkpoint_X/checkpoint-X\"\n",
    "#      PARAMS = DIR + \"params.pkl\"\n",
    "\n",
    "PPO_DIR = \"pretrained_results/PPO/\"\n",
    "PPO_CHECKPOINT = PPO_DIR + \"checkpoint_500/checkpoint-500\"\n",
    "PPO_PARAMS = PPO_DIR + \"params.pkl\"\n",
    "\n",
    "STEPS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_frames = run_rollout(checkpoint=PPO_CHECKPOINT, params_path=PPO_PARAMS, run_algo=\"PPO\", env=\"train_env\", steps=STEPS)\n",
    "animate_env(ppo_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to CCPPO.\n",
    "\n",
    "In our trials, we find that CCPPO can solve this environment, while PPO cannot.  \n",
    "We also note that CCPPO converges in approximately half the time.  \n",
    "How did your experiments do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCPPO checkpoint: update this as the path to your trained CCPPO model\n",
    "CCPPO_DIR = \"pretrained_results/CCPPO/\"\n",
    "CCPPO_CHECKPOINT = CCPPO_DIR + \"checkpoint_500/checkpoint-500\"\n",
    "CCPPO_PARAMS = CCPPO_DIR + \"params.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccppo_frames = run_rollout(checkpoint=CCPPO_CHECKPOINT, params_path=CCPPO_PARAMS, run_algo=\"PPO\", env=\"train_env\", steps=STEPS)\n",
    "animate_env(ccppo_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained Model Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.cloud.google.com/gtc-2020/images/episode_reward_mean.png\" width=\"800\" height=\"600\">\n",
    "<img src=\"https://storage.cloud.google.com/gtc-2020/images/episode_reward_mean_key.png\" width=\"800\" height=\"600\">\n",
    "\n",
    "<img src=\"https://storage.cloud.google.com/gtc-2020/images/episode_reward_max.png\" width=\"800\" height=\"600\">\n",
    "<img src=\"https://storage.cloud.google.com/gtc-2020/images/episode_reward_max_key.png\" width=\"800\" height=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fl_test] *",
   "language": "python",
   "name": "conda-env-fl_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
