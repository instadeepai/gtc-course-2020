{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5H5GxUed8Di"
   },
   "source": [
    "## Introduction to Flatland for MultiAgent Reinforcement Learning on Railway Environments  \n",
    "\n",
    "In this notebook, we will learn how to:  \n",
    "   * Create Flatland railway environments\n",
    "   * Use TensorFlow to build a neural network for a reinforcement learning agent\n",
    "   * Take actions and visualize agents in the environment\n",
    "   \n",
    "The main aim is to introduce reinforcement learning concepts and understand different parts of the problem.  \n",
    "In the next notebook, we will introduce a framework to solve problems at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flatland.envs.observations import GlobalObsForRailEnv\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from environments.custom_rail_generator import simple_rail_generator\n",
    "from environments.custom_schedule_generator import sparse_schedule_generator\n",
    "from environments.env_utils import env_from_env_config\n",
    "from environments.observations import TreeObsForRailEnv\n",
    "from environments.visualization_utils import animate_env, get_patch, render_env\n",
    "\n",
    "# Docker\n",
    "# Start virtual display before importing RenderTool\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "\n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Here we generate a random environment for each episode, with characteristics specified by the environment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENVIRONMENT SETUP - \n",
    "n_trains = 3\n",
    "env_config = {\n",
    "    \"obs_config\": {\"max_depth\": 2},\n",
    "    \"rail_generator\": \"complex_rail_generator\",\n",
    "    \"rail_config\": {\"nr_start_goal\": 12, \"nr_extra\": 0, \"min_dist\": 8, \"seed\": 10},\n",
    "    \"width\": 8,\n",
    "    \"height\": 8,\n",
    "    \"number_of_agents\": n_trains,\n",
    "    \"schedule_generator\": \"complex_schedule_generator\",\n",
    "    \"schedule_config\": {},\n",
    "    \"frozen\": False,\n",
    "    \"remove_agents_at_target\": True,\n",
    "    \"wait_for_all_done\": False\n",
    "}\n",
    "env = env_from_env_config(env_config)\n",
    "\n",
    "observation_space = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the previous section, we discussed how the Flatland observation space is arranged in a tree structure.  \n",
    "* While this is convenient for scalable architectures, we need to supply inputs to our neural networks in vector format.  \n",
    "* To achieve this, we use a preprocessor to transform into the required representation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.preprocessor import TreeObsPreprocessor\n",
    "from gym.spaces import Box\n",
    "\n",
    "preprocessor = TreeObsPreprocessor(Box(low=-np.inf, high=np.inf, shape=(observation_space,)), \n",
    "                                       {\"custom_options\": {\"tree_depth\": 2, \"observation_radius\": 0}})\n",
    "\n",
    "def preprocess(observation):\n",
    "    \"\"\"\n",
    "    Tree-like observations --> vector of observations to feed as input to our neural network\n",
    "    \"\"\"\n",
    "    observation = [preprocessor.transform(o) for o in list(observation.values())]\n",
    "    return np.concatenate(observation, axis=-1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "We are going to use TensorFlow to implement a neural network.  \n",
    "* We will receive observations $o_t$ of the state of the environment, $s_t$ at time $t$\n",
    "* We will use the neural network as a function approximator to map $o_t$ to the state-action value $Q(o_t, a_t)$\n",
    "* With this, we can define a policy for the agent to take actions in the environment based on state-action value estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers, losses, Model, optimizers\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify the code in the cell below to implement your own neural network\n",
    "* You can add hidden layers by changing the number of units in the `hiddens` array\n",
    "* Remember to specify `activations` for each layer\n",
    "* You might also like to consider a different architecture to a fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFModel(Model):\n",
    "    def __init__(self, n_actions, n_trains):\n",
    "        super(TFModel, self).__init__()\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        # TODO: prepend some layers\n",
    "        hiddens = [n_actions**n_trains]\n",
    "        activations = [None]\n",
    "        \n",
    "        self.dense_layers = [\n",
    "            Dense(\n",
    "                h,\n",
    "                name=f\"fc_{k}\",\n",
    "                activation=activation,\n",
    "                kernel_initializer=initializers.glorot_normal(),\n",
    "                bias_initializer=initializers.constant(0.1),\n",
    "            )\n",
    "            for k, (h, activation) in enumerate(\n",
    "                zip(hiddens, activations)\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Introduction\n",
    "\n",
    "For this demonstration of centralized control, we will make use of a simple DQN agent.  \n",
    "\n",
    "DQN is a value iteration algorithm, which also makes use of frozen target networks and replay buffers.\n",
    "  * The target networks improve learning stability by preventing us from having to train on a moving target.\n",
    "  * Trajectories are stored in the replay buffer, and later sampled to facilitate off-policy training on decorrelated data.  \n",
    "\n",
    "These details are not the focus of this course, but more information can be found in Google DeepMind's original paper:  \n",
    "https://arxiv.org/abs/1312.5602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self):\n",
    "        self.tau=0.01  # Updating target hyperparameter\n",
    "        self.model = TFModel(n_actions, n_trains)  # Behavioural Model\n",
    "        self.target_model = TFModel(n_actions, n_trains)  # Frozen Target Model\n",
    "        self.target_model.set_weights(self.model.get_weights())  # Clone\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "            loss=losses.MeanSquaredError()\n",
    "        )\n",
    "        \n",
    "    def update_target_networks(self):\n",
    "        \"\"\"theta_target = (1-tau)*theta_target + tau*theta\"\"\"\n",
    "        model_params = self.model.get_weights()\n",
    "        target_model_params = self.target_model.get_weights()\n",
    "\n",
    "        index = 0\n",
    "        for m, t in zip(model_params, target_model_params):\n",
    "            t = t * (1 - self.tau) + m * self.tau\n",
    "            target_model_params[index] = t\n",
    "            index += 1\n",
    "\n",
    "        self.target_model.set_weights(target_model_params)\n",
    "    \n",
    "dqn_agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN is a value based algorithm: our neural network will output state-action values, $Q(s,a)$.\n",
    "\n",
    "\n",
    "To know how to behave, we must define an explicit policy.  \n",
    "* In this case, the best policy we can define is a greedy one, always taking the action corresponding to the state-action pair with the highest Q-value.\n",
    "* We encourage some exploration by occasionally ignoring the greedy policy and taking a random action.\n",
    "* This arrives at our policy of choice for this algorithm: $\\epsilon$-greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(Q, epsilon):\n",
    "    \"\"\"Epsilon greedy policy.\"\"\"\n",
    "    \n",
    "    # Random action\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return np.random.randint(0, n_actions**n_trains)\n",
    "    \n",
    "    # Greedy action\n",
    "    return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a single agent to control all of the trains in a centralized manner, we need to translate the action index output by the $\\epsilon$-greedy policy into a tuple of numbers that represent a real action to be taken by each train in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions_per_train(action):\n",
    "    \"\"\"\n",
    "    E.g. for 3 trains, each with 5 actions:\n",
    "         a model output of 11 --> (0,2,1).\n",
    "         I.e. train 1 takes action 0,\n",
    "              train 2 takes action 2,\n",
    "              train 3 takes action 1.\n",
    "    \"\"\"\n",
    "    return np.unravel_index(action, [n_actions] * n_trains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all trains enter and exit the environment in the same time step.  \n",
    "We will import some helper functions to pad any missing observations with zeros.  \n",
    "These values will not affect the training of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from environments.env_utils import pad_initial_obs, pad_done_agents\n",
    "\n",
    "pad_obs = functools.partial(pad_initial_obs, observation_space=observation_space, n_trains=n_trains)\n",
    "pad_dones = functools.partial(pad_done_agents, observation_space=observation_space, n_trains=n_trains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.cloud.google.com/gtc-2020/images/DQN_algorithm.png\" width=\"500\" height=\"400\" align=\"left\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Image from https://arxiv.org/abs/1312.5602]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below is the first component of the DQN algorithm, which gets trajectories of experience in the environment\n",
    "* Given an observation of the state of the environment, you need to use your model to make a `prediction` of the state-action values\n",
    "* Based on the predicted Q-values, the agent takes an action, $a_t$, for each train in the environment according to an $\\epsilon$-greedily policy\n",
    "* The environment transitions into the next state, $s_{t+1}$, returning rewards, $r_t$, for the action taken for each train\n",
    "\n",
    "These trajectories are highly correlated with each other, due to the sequential nature of their acquisition. For this reason, we do not train on them immediately, but prefer to store them in a `replay buffer`, for off-policy training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(obs, epsilon):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        obs: current observation of the state of trains in the environment\n",
    "        epsilon: current hyperparameter value for exploration\n",
    "    Returns:\n",
    "        trajectory of (obs, actions, rewards, next_obs, dones)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: use your neural network model to make a prediction of Q-values\n",
    "    # given the observation, obs, of the current state of the environment\n",
    "    # Q = ...\n",
    "    \n",
    "    # Epsilon greedy policy\n",
    "    actions = e_greedy(np.squeeze(Q), epsilon=epsilon)\n",
    "    \n",
    "    # Convert single index to action for each train\n",
    "    a = actions_per_train(actions)\n",
    "    \n",
    "    # Flatland env expects a dictionary of actions\n",
    "    action_dict = dict(zip(list(np.arange(len(a))), list(a)))\n",
    "\n",
    "    # Take a step in the environment\n",
    "    next_obs, rewards, dones, info = env.step(action_dict)\n",
    "    \n",
    "    next_obs = preprocess(next_obs)\n",
    "    next_obs, rewards, dones = pad_dones(next_obs, rewards, dones)\n",
    "    \n",
    "    return obs, actions, rewards, next_obs, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must consider how to train our agent\n",
    "\n",
    "* In the cell below is a function `train_dqn`, which receives trajectories of experience sampled from the replay buffer\n",
    "* The neural network is trained off-policy\n",
    "* This is achieved using the target model to make a prediction of the target Q-value of the next state-action pair, $Q(\\phi_{j+1}, a'; \\theta)$\n",
    "* The behavorial model parameters are then adjusted through backpropagation to correct the current Q-value towards the value of the immediate reward received + the discounted future prediction as, specified by the Bellman Optimality Equation:\n",
    "\n",
    "$y_j = r_j + \\gamma max_{a'} Q(\\phi_{j+1}, a'; \\theta)$\n",
    "\n",
    "You will need to implement this equation in the training loop using the values predicted from your target Q model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(trajectories, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        trajectories: batch of (obs, actions, rewards, next_obs, dones)\n",
    "        gamma: future rewards discount factor\n",
    "    \"\"\"\n",
    "    obs, actions, rewards, next_obs, dones = trajectories\n",
    "\n",
    "    # TODO: use the target_model to make a prediction of the target Q for the next_obs_batch\n",
    "    # Q_target = ...\n",
    "\n",
    "    # Initialize training target\n",
    "    y = np.zeros_like(Q_target)\n",
    "    for k in range(batch_size):\n",
    "\n",
    "        # We take the mean of rewards for all trains, to maintain r in [-1,1]\n",
    "        r = sum(rewards[k]) / n_trains\n",
    "        \n",
    "        # We need to update the Q(s,a) value for the action taken\n",
    "        action = actions[k]\n",
    "\n",
    "        if dones[k]['__all__']:\n",
    "            y[k][action] = r\n",
    "        else:\n",
    "            # TODO: fill in y_batch[k][action] using the Bellman Equation (see above).\n",
    "            # (Hint: gamma has been set at the top of this cell.)\n",
    "            # y_batch[k][action] = ...\n",
    "\n",
    "            \n",
    "    # Gradient Descent on (y - Q)^2\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        Q = dqn_agent.model(obs)\n",
    "        \n",
    "        # Compute the loss on the actions taken\n",
    "        actions = tf.one_hot(actions, n_actions**n_trains)\n",
    "        loss = dqn_agent.model.loss(Q * actions, y)\n",
    "        \n",
    "    variables = dqn_agent.model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    dqn_agent.model.optimizer.apply_gradients(zip(gradients, variables))\n",
    "    dqn_agent.update_target_networks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training loop\n",
    "\n",
    "Run the cell below to execute training across multiple episodes of game play in the environment\n",
    "* For best results, train for longer by increasing `n_episodes`\n",
    "* For quicker results, decrease `n_episodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a replay buffer for storing training trajectories\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# Training parameters\n",
    "n_episodes = 100\n",
    "max_steps_per_episode = 25\n",
    "\n",
    "# Mini-batch size of trajectory samples\n",
    "batch_size = 128\n",
    "\n",
    "# Exploration\n",
    "epsilon = 0.99\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.0\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    obs = preprocess(obs)\n",
    "    obs = pad_obs(obs)\n",
    "\n",
    "    episode_reward = 0\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        \n",
    "        # Decay exploration coefficient\n",
    "        epsilon = max(min_epsilon, epsilon*epsilon_decay)\n",
    "        \n",
    "        # Store trajectory experience tuple in replay memory\n",
    "        obs, actions, rewards, next_obs, dones = get_trajectory(obs, epsilon)\n",
    "        replay_buffer.add(obs, actions, rewards, next_obs, dones)\n",
    "        \n",
    "        # Update tally of current episode reward\n",
    "        episode_reward += sum(rewards)\n",
    "        \n",
    "        if replay_buffer.size() > batch_size:\n",
    "            # Sample de-correlated mini-batch of trajectories\n",
    "            trajectories = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            # Train DQN agent with the function you wrote above\n",
    "            train_dqn(trajectories)\n",
    "        \n",
    "        if dones['__all__']:\n",
    "            break\n",
    "\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Episode {episode+1} Reward: {episode_reward/n_trains:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Performance  \n",
    "\n",
    "* For this demonstration, we will visualize performance on a fixed test environment\n",
    "* This test environment was not seen during training\n",
    "* The test city map is also larger than the training maps\n",
    "* After enough training, this can be used to test our agent's ability to generalize to new scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_ration_map = {1.: 1.}\n",
    "rail_generator = simple_rail_generator(n_trains=n_trains, seed=42)\n",
    "schedule_generator = sparse_schedule_generator(speed_ration_map)\n",
    "obs_builder_object = TreeObsForRailEnv(max_depth=2, predictor=None)\n",
    "\n",
    "env = RailEnv(\n",
    "            width=16,\n",
    "            height=16,\n",
    "            number_of_agents=n_trains,\n",
    "            rail_generator=rail_generator,\n",
    "            schedule_generator=schedule_generator,\n",
    "            obs_builder_object=obs_builder_object,\n",
    "            remove_agents_at_target=True,\n",
    "        )\n",
    "\n",
    "# Instantiate Renderer\n",
    "env_renderer = RenderTool(env, gl=\"PILSVG\",\n",
    "                          agent_render_variant=AgentRenderVariant.AGENT_SHOWS_OPTIONS_AND_BOX,\n",
    "                          show_debug=False,\n",
    "                          screen_height=726,\n",
    "                          screen_width=1240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs = preprocess(obs[0])\n",
    "\n",
    "env_renderer.reset()\n",
    "\n",
    "frames = []\n",
    "n_actions = env.action_space[0]\n",
    "\n",
    "rollout_steps = 25\n",
    "for step in range(rollout_steps):\n",
    "    \n",
    "    Q = dqn_agent.model.predict(np.expand_dims(obs, 0))\n",
    "    \n",
    "    # Test with a deterministic policy: epsilon=0\n",
    "    actions = e_greedy(np.squeeze(Q, axis=0), epsilon=0)\n",
    "    \n",
    "    a = actions_per_train(actions)\n",
    "    action_dict = dict(zip(list(np.arange(len(a))), list(a)))\n",
    "\n",
    "    obs, rewards, dones, info = env.step(action_dict)\n",
    "    obs = preprocess(obs)\n",
    "\n",
    "    env_renderer.render_env(show=False, frames=False, show_observations=False)\n",
    "    frames.append(env_renderer.gl.get_image())\n",
    "    \n",
    "animate_env(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability: from centralized control to MARL\n",
    "\n",
    "In this section:\n",
    "* We illustrated an example of the centralized control of multiple trains\n",
    "\n",
    "* We learned how to use TensorFlow to build and train a neural network as a function approximator to help in this task\n",
    "\n",
    "\n",
    "* In general, this approach works well, but it is not scalable:  \n",
    "notice how, for N actions and t trains, our neural network had to produce $N^t$ values for a joint action space!\n",
    "\n",
    "* This is manageable in our example with 3 trains, but what if we wanted to scale to 100s or 1000s of trains?\n",
    "\n",
    "\n",
    "In the next section:\n",
    "* We will explore multi-agent reinforcement learning as a more scalable alternative\n",
    "\n",
    "* We will introduce the RLlib framework to help train more advanced algorithms than our naive implementation of DQN\n",
    "\n",
    "* This will improve utilization of available compute resources"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "env_viz.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "instadeep-gtc",
   "language": "python",
   "name": "instadeep-gtc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
