{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Reinforcement Learning with Centralized Critic\n",
    "\n",
    "* A major assumption of reinforcement learning is that the states of the environment obey the Markov property.\n",
    "* In the multi-agent setting, this is not strictly true.\n",
    "* From the perspective of one agent, all the other agents are considered part of the environment.\n",
    "* However, the other agents are learning to improve their policies over time, meaning the environment is no longer stationary from the perspective of any given agent.  \n",
    "* In adversarial settings, this can lead to catastophic collapses of performance, as the agent fails to respond correctly to its opponent.\n",
    "* In collaborative settings such as ours, we can run into issues when assigning credit (or blame) for events like collisions or gridlock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.cloud.google.com/gtc-2020/images/gridlock.png\">\n",
    "[https://bair.berkeley.edu/blog/2018/12/12/rllib/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One method of alleviating this problem is to share the observations and actions of each agent during training.\n",
    "* In the previous sections, we implemented the PPO algorithm in a decentralized multi-agent setting.\n",
    "* In this section, we will run PPO, but this time with a *centralized critic*.\n",
    "* We call this new algorithm CCPPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env\n",
    "from environments.env_utils import env_from_env_config\n",
    "from environments.preprocessor import TreeObsPreprocessor\n",
    "from models.centralized_critic_model import CentralizedCriticModel\n",
    "from models.centralized_critic_policy import CCTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dense model with \"u\" hidden_layer units per layer\n",
    "u = 10\n",
    "model_params = {\n",
    "    \"embedding\": {\"hidden_layers\": [u, u], \"activation_fn\": \"relu\"},\n",
    "    \"actor\": {\"hidden_layers\": [u], \"activation_fn\": \"relu\"},\n",
    "    \"critic\": {\"hidden_layers\": [u], \"activation_fn\": \"relu\"},\n",
    "    \"central_vf_size\": u,\n",
    "}\n",
    "custom_model = \"cc_dense\"\n",
    "\n",
    "# Set up the environment\n",
    "env_config = {\n",
    "    \"obs_config\": {\"max_depth\": 2},\n",
    "    \"rail_generator\": \"complex_rail_generator\",\n",
    "    \"rail_config\": {\"nr_start_goal\": 12,\"nr_extra\": 0, \"min_dist\": 8, \"seed\": 10},\n",
    "    \"width\": 8,\n",
    "    \"height\": 8,\n",
    "    \"number_of_agents\": 3,\n",
    "    \"schedule_generator\": \"complex_schedule_generator\",\n",
    "    \"schedule_config\": {},\n",
    "    \"frozen\": False,\n",
    "    \"remove_agents_at_target\": True,\n",
    "    \"wait_for_all_done\": True\n",
    "}\n",
    "tmp_env = env_from_env_config(env_config)\n",
    "action_space = tmp_env.action_space\n",
    "observation_space = tmp_env.observation_space\n",
    "\n",
    "# Define 1 policy per agent\n",
    "num_policies = env_config[\"number_of_agents\"]\n",
    "policies = {f\"policy_{i}\": (None, observation_space, action_space, {\"agent_id\": i})\n",
    "            for i in range(num_policies)}\n",
    "policy_ids = list(policies.keys())\n",
    "\n",
    "# Register custom setup with RLlib\n",
    "register_env(\"train_env\", env_from_env_config)\n",
    "ModelCatalog.register_custom_model(custom_model, CentralizedCriticModel)\n",
    "ModelCatalog.register_custom_preprocessor(\"tree_obs_preprocessor\", TreeObsPreprocessor)\n",
    "\n",
    "# Full experiment config\n",
    "config = {\n",
    "    # Run parameters\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_workers\": 7,\n",
    "    \"num_gpus\": 0,  # TODO: change this to 1\n",
    "    \n",
    "    # Environment parameters\n",
    "    \"env\": \"train_env\",\n",
    "    \"env_config\": env_config,\n",
    "    \"log_level\": \"ERROR\",\n",
    "    \n",
    "    # Training parameters\n",
    "    \"horizon\": 40,\n",
    "    \"num_sgd_iter\": 15,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_mode\": \"complete_episodes\",\n",
    "    \n",
    "    # Policy parameters\n",
    "    \"vf_loss_coeff\": 1e-6,    \n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: policy_ids[agent_id]\n",
    "    },\n",
    "    \n",
    "    # Model parameters\n",
    "    \"model\": {\n",
    "        \"custom_preprocessor\": \"tree_obs_preprocessor\",\n",
    "        \"custom_model\": custom_model,\n",
    "        \"custom_options\": {\n",
    "            \"tree_depth\": env_config[\"obs_config\"][\"max_depth\"],\n",
    "            \"observation_radius\": 0,\n",
    "            \"max_num_agents\": env_config[\"number_of_agents\"],\n",
    "            **model_params,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start TensorBoard\n",
    "* You should see the training results from the previous notebook.\n",
    "* CCPPO will typically learn cooperative strategies faster than decentralized PPO.\n",
    "* Results can vary at small scales, but we will see results from slightly larger runs in the following notebook.\n",
    "* While your models train, we will move to the final section to visualize some rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=~/ray_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    CCTrainer,\n",
    "    name=f\"CCPPO-MODEL_{custom_model}_{u}\",\n",
    "    stop={\"training_iteration\": 80},\n",
    "    config=config,\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True,\n",
    "    loggers=tune.logger.DEFAULT_LOGGERS,\n",
    "    ray_auto_init=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fl_test] *",
   "language": "python",
   "name": "conda-env-fl_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
