{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Reinforcement Learning\n",
    "\n",
    "In the previous section, we discussed how to build a model for centalized control.\n",
    "   * When trained properly, this approach can work very well.\n",
    "   \n",
    "   * However, we saw an exponential increase in the of the joint action space as the number of trains in the environment increased.\n",
    "     As a result, this approach will be too severely limited to work at scale with a large number of trains.\n",
    "     \n",
    "   * Instead, we reformulate the objective as a multi-agent reinforcement learning task.\n",
    "     * This is a more natural representation of the problem, in which each train operates as a decentralized agent.\n",
    "     * That is to say, from the perspective of one agent, all other agents are part of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.env_utils import env_from_env_config\n",
    "from environments.observations import TreeObsForRailEnv\n",
    "from environments.preprocessor import TreeObsPreprocessor\n",
    "from models.dense_model import DenseModel\n",
    "from ray import tune\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using RLlib, a scalable reinforcement learning library built on top of Ray by the Berkeley AI Research Group\n",
    "\n",
    "\n",
    "With relatively few lines of additional code, Ray enables deep learning and reinforcement learning practitioners to turn their prototype algorithms into distributed production scale solutions trained on clusters.  \n",
    "\n",
    "RLlib is a highly scalable library of reinforcement learning algorithms which natively supports popular deep learning frameworks such as TensorFlow and PyTorch.  \n",
    "\n",
    "You can find out more about Ray here: https://bair.berkeley.edu/blog/2018/01/09/ray/  \n",
    "and more on RLlib here: https://ray.readthedocs.io/en/latest/rllib.html  \n",
    "\n",
    "\n",
    "Reinforcement learning algorithms are hungry for training examples, making training in a multi-GPU setting very important for the speed of convergence, especially at scale. We will see how RLlib's support of CUDA devices leverages Nvidia GPUs to offer a significant reduction in time taken to train.  \n",
    "\n",
    "More information on the importance of GPUs for use with RLlib can be found here:  \n",
    "https://ray.readthedocs.io/en/latest/using-ray-with-gpus.html\n",
    "\n",
    "But for now, let's train! \n",
    "\n",
    "Run the cell below to set up the first experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dense model with \"u\" hidden_layer units per layer\n",
    "u = 128\n",
    "model_params = {\n",
    "    \"embedding\": {\"hidden_layers\": [u, u], \"activation_fn\": \"relu\"},\n",
    "    \"actor\": {\"hidden_layers\": [u], \"activation_fn\": \"relu\"},\n",
    "    \"critic\": {\"hidden_layers\": [u], \"activation_fn\": \"relu\"},\n",
    "}\n",
    "custom_model = \"dense_model\"\n",
    "\n",
    "# Set up the environment\n",
    "env_config = {\n",
    "    \"obs_config\": {\"max_depth\": 2},\n",
    "    \"rail_generator\": \"complex_rail_generator\",\n",
    "    \"rail_config\": {\"nr_start_goal\": 12, \"nr_extra\": 0, \"min_dist\": 8, \"seed\": 10},\n",
    "    \"width\": 8,\n",
    "    \"height\": 8,\n",
    "    \"number_of_agents\": 5,\n",
    "    \"schedule_generator\": \"complex_schedule_generator\",\n",
    "    \"schedule_config\": {},\n",
    "    \"frozen\": False,\n",
    "    \"remove_agents_at_target\": True,\n",
    "    \"wait_for_all_done\": False\n",
    "}\n",
    "env = env_from_env_config(env_config)\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "# Define 1 policy per agent\n",
    "num_policies = env_config[\"number_of_agents\"]\n",
    "policies = {f\"policy_{i}\": (None, observation_space, action_space, {})\n",
    "            for i in range(num_policies)}\n",
    "\n",
    "# Register custom setup with RLlib\n",
    "register_env(\"train_env\", env_from_env_config)\n",
    "ModelCatalog.register_custom_model(custom_model, DenseModel)\n",
    "ModelCatalog.register_custom_preprocessor(\"tree_obs_preprocessor\", TreeObsPreprocessor)\n",
    "\n",
    "# Full experiment config\n",
    "config = {\n",
    "    # Run parameters\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \n",
    "    # Environment parameters\n",
    "    \"env\": \"train_env\",\n",
    "    \"env_config\": env_config,\n",
    "    \"log_level\": \"ERROR\",\n",
    "    \n",
    "    # Training parameters\n",
    "    \"horizon\": 60,\n",
    "    \"num_sgd_iter\": 15,\n",
    "    \"lr\": 1e-4,\n",
    "    \n",
    "    # Policy parameters\n",
    "    \"vf_loss_coeff\": 1e-6,    \n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"policy_0\",\n",
    "    },\n",
    "    \n",
    "    # Model parameters\n",
    "    \"model\": {\n",
    "        \"custom_preprocessor\": \"tree_obs_preprocessor\",\n",
    "        \"custom_model\": custom_model,\n",
    "        \"custom_options\": {\n",
    "            \"tree_depth\": env_config[\"obs_config\"][\"max_depth\"],\n",
    "            \"observation_radius\": 0,\n",
    "            **model_params,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with CPU vs. GPU\n",
    "\n",
    "* We are going to compare speed of training with and without the GPUs.  \n",
    "* At the scale of environment we are running today, we should see a difference within a few minutes of training.\n",
    "* At larger scales, such as a full sized rail network, the difference in speed is much more pronounced.\n",
    "\n",
    "Let's get started by loading TensorBoard while we discuss the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=~/ray_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to launch a short training run, using only CPUs.\n",
    "* Then change the number of GPUs to 4 and re-run the cell.\n",
    "* Check the difference in TensorBoard. (Hint: change the smoothing to help visualize results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parameters\n",
    "config[\"num_workers\"] = 7\n",
    "config[\"num_gpus\"] = 0  # TODO: change this to 1\n",
    "\n",
    "n_GPUS = config[\"num_gpus\"]\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    name=f\"PPO_multi_agent-MODEL={custom_model}_{u}-GPUS={n_GPUS}\",\n",
    "    stop={\"episode_reward_mean\": -18,\n",
    "         \"training_iteration\": 50},\n",
    "    config=config,\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True,\n",
    "    loggers=tune.logger.DEFAULT_LOGGERS,\n",
    "    ray_auto_init=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fl_test] *",
   "language": "python",
   "name": "conda-env-fl_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
